{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請實做以下兩種不同feature的模型，回答第 (1) ~ (3) 題：\n",
    "\n",
    "(1) 抽全部9小時內的污染源feature當作一次項(加bias)  \n",
    "(2) 抽全部9小時內pm2.5的一次項當作feature(加bias)\n",
    "\n",
    "    備註 : \n",
    "      1. NR請皆設為0，其他的數值不要做任何更動\n",
    "      2. 所有 advanced 的 gradient descent 技術(如: adam, adagrad 等) 都是可以用的\n",
    "      3. 第1-3題請都以題目給訂的兩種model來回答\n",
    "      4. 同學可以先把model訓練好，kaggle死線之後便可以無限上傳。\n",
    "      5. 根據助教時間的公式表示，(1) 代表 p = 9x18+1 而(2) 代表 p = 9*1+1\n",
    "      \n",
    "1. (2%)記錄誤差值 (RMSE)(根據kaggle public+private分數)，討論兩種feature的影響\n",
    "\n",
    "2. (1%)將feature從抽前9小時改成抽前5小時，討論其變化\n",
    "\n",
    "3. (1%)Regularization on all the weight with λ=0.1、0.01、0.001、0.0001，並作圖\n",
    "      \n",
    "4. (1%)在線性回歸問題中，假設有 N 筆訓練資料，每筆訓練資料的特徵 (feature) 為一向量 xn，其標註(label)為一純量 yn，模型參數為一向量w (此處忽略偏權值 b)，則線性回歸的損失函數(loss function)為n=1Nyn-xnw2 。若將所有訓練資料的特徵值以矩陣 X = [x1 x2 … xN]T 表示，所有訓練資料的標註以向量 y = [y1 y2 … yN]T表示，請問如何以 X 和 y 表示可以最小化損失函數的向量 w ？請選出正確答案。(其中XTX為invertible)\n",
    "\n",
    "        (A) (XTX)XTy\n",
    "        (B) (XTX)yXT\n",
    "        (C) (XTX)-1XTy\n",
    "        (D) (XTX)-1yXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ml2019spring-hw1/sampleSubmission.csv\n",
      "/kaggle/input/ml2019spring-hw1/train.csv\n",
      "/kaggle/input/ml2019spring-hw1/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 抽全部9小時內的污染源feature當作一次項(加bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5652, 162), (5652, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = np.genfromtxt('../input/ml2019spring-hw1/train.csv', encoding = 'unicode_escape', delimiter=',') ## train.csv\n",
    "data = raw_data[1:,3:]\n",
    "where_are_NaNs = np.isnan(data)\n",
    "data[where_are_NaNs] = 0 \n",
    "\n",
    "month_to_data = {}  ## Dictionary (key:month , value:data)                                  \n",
    "\n",
    "for month in range(12):\n",
    "    sample = np.empty(shape = (18 , 480))\n",
    "    for day in range(20):\n",
    "        for hour in range(24): \n",
    "            sample[:,day * 24 + hour] = data[18 * (month * 20 + day): 18 * (month * 20 + day + 1),hour]\n",
    "    month_to_data[month] = sample  \n",
    "x = np.empty(shape = (12 * 471 , 18 * 9),dtype = float)\n",
    "y = np.empty(shape = (12 * 471 , 1),dtype = float)\n",
    "\n",
    "for month in range(12): \n",
    "    for day in range(20): \n",
    "        for hour in range(24):   \n",
    "            if day == 19 and hour > 14:\n",
    "                continue\n",
    "            x[month * 471 + day * 24 + hour,:] = month_to_data[month][:,day * 24 + hour : day * 24 + hour + 9].reshape(1,-1) \n",
    "            y[month * 471 + day * 24 + hour,0] = month_to_data[month][9 ,day * 24 + hour + 9]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(x, axis = 0) \n",
    "std = np.std(x, axis = 0)\n",
    "for i in range(x.shape[0]):\n",
    "    for j in range(x.shape[1]):\n",
    "        if not std[j] == 0 :\n",
    "            x[i][j] = (x[i][j]- mean[j]) / std[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T= 0\n",
      "Loss: 27.071214829194115\n",
      "T= 500\n",
      "Loss: 15.303358672239398\n",
      "T= 1000\n",
      "Loss: 9.957336253580277\n",
      "T= 1500\n",
      "Loss: 7.879329888549475\n",
      "T= 2000\n",
      "Loss: 6.861485712171394\n",
      "T= 2500\n",
      "Loss: 6.339003480940456\n",
      "T= 3000\n",
      "Loss: 6.060560050269416\n",
      "T= 3500\n",
      "Loss: 5.9069391516050365\n",
      "T= 4000\n",
      "Loss: 5.819427069726108\n",
      "T= 4500\n",
      "Loss: 5.768117525889319\n",
      "T= 5000\n",
      "Loss: 5.737251324218732\n",
      "T= 5500\n",
      "Loss: 5.718247893619341\n",
      "T= 6000\n",
      "Loss: 5.70629462196756\n",
      "T= 6500\n",
      "Loss: 5.698621259127973\n",
      "T= 7000\n",
      "Loss: 5.693596789769688\n",
      "T= 7500\n",
      "Loss: 5.690241610223954\n",
      "T= 8000\n",
      "Loss: 5.687956629045993\n",
      "T= 8500\n",
      "Loss: 5.686369228004013\n",
      "T= 9000\n",
      "Loss: 5.685243884510883\n",
      "T= 9500\n",
      "Loss: 5.684429390954337\n"
     ]
    }
   ],
   "source": [
    "dim = x.shape[1] + 1 \n",
    "w = np.zeros(shape = (dim, 1 ))\n",
    "x = np.concatenate((np.ones((x.shape[0], 1 )), x) , axis = 1).astype(float)\n",
    "learning_rate = np.array([[200]] * dim)\n",
    "adagrad_sum = np.zeros(shape = (dim, 1 ))\n",
    " \n",
    "for T in range(10000):\n",
    "    if(T % 500 == 0 ):\n",
    "        print(\"T=\",T)\n",
    "        print(\"Loss:\",np.power(np.sum(np.power(x.dot(w) - y, 2 ))/ x.shape[0],0.5))\n",
    "    gradient = (-2) * np.transpose(x).dot(y-x.dot(w))\n",
    "    adagrad_sum += gradient ** 2\n",
    "    w = w - learning_rate * gradient / (np.sqrt(adagrad_sum) + 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_data = np.genfromtxt('../input/ml2019spring-hw1/test.csv', encoding = 'unicode_escape', delimiter=',')   ## test.csv\n",
    "test_data = test_raw_data[:, 2: ]\n",
    "where_are_NaNs = np.isnan(test_data)\n",
    "test_data[where_are_NaNs] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.empty(shape = (240, 18 * 9),dtype = float)\n",
    "\n",
    "for i in range(240):\n",
    "    test_x[i,:] = test_data[18 * i : 18 * (i+1),:].reshape(1,-1) \n",
    "\n",
    "for i in range(test_x.shape[0]):        ##Normalization\n",
    "    for j in range(test_x.shape[1]):\n",
    "        if not std[j] == 0 :\n",
    "            test_x[i][j] = (test_x[i][j]- mean[j]) / std[j]\n",
    "\n",
    "test_x = np.concatenate((np.ones(shape = (test_x.shape[0],1)),test_x),axis = 1).astype(float)\n",
    "answer = test_x.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('submission.csv',\"w\")\n",
    "w = csv.writer(f)\n",
    "title = ['id','value']\n",
    "w.writerow(title) \n",
    "for i in range(240):\n",
    "    content = ['id_'+str(i),answer[i][0]]\n",
    "    w.writerow(content) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> score:5.65650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 抽全部9小時內pm2.5的一次項當作feature(加bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5652, 9), (5652, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = np.genfromtxt('../input/ml2019spring-hw1/train.csv', encoding = 'unicode_escape', delimiter=',') ## train.csv\n",
    "data = raw_data[1:,3:]\n",
    "where_are_NaNs = np.isnan(data)\n",
    "data[where_are_NaNs] = 0 \n",
    "\n",
    "month_to_data = {}  ## Dictionary (key:month , value:data)                                  \n",
    "\n",
    "for month in range(12):\n",
    "    sample = np.empty(shape = (18 , 480))\n",
    "    for day in range(20):\n",
    "        for hour in range(24): \n",
    "            sample[:,day * 24 + hour] = data[18 * (month * 20 + day): 18 * (month * 20 + day + 1),hour]\n",
    "    month_to_data[month] = sample  \n",
    "x = np.empty(shape = (12 * 471 , 9),dtype = float)\n",
    "y = np.empty(shape = (12 * 471 , 1),dtype = float)\n",
    "\n",
    "for month in range(12): \n",
    "    for day in range(20): \n",
    "        for hour in range(24):   \n",
    "            if day == 19 and hour > 14:\n",
    "                continue\n",
    "            x[month * 471 + day * 24 + hour,:] = month_to_data[month][9, day * 24 + hour : day * 24 + hour + 9].reshape(1,-1) \n",
    "            y[month * 471 + day * 24 + hour,0] = month_to_data[month][9, day * 24 + hour + 9]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(x, axis = 0) \n",
    "std = np.std(x, axis = 0)\n",
    "for i in range(x.shape[0]):\n",
    "    for j in range(x.shape[1]):\n",
    "        if not std[j] == 0 :\n",
    "            x[i][j] = (x[i][j]- mean[j]) / std[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T= 0\n",
      "Loss: 27.071214829194115\n",
      "T= 500\n",
      "Loss: 6.1332103618860625\n",
      "T= 1000\n",
      "Loss: 6.123265003043738\n",
      "T= 1500\n",
      "Loss: 6.123028371018228\n",
      "T= 2000\n",
      "Loss: 6.123021754567614\n",
      "T= 2500\n",
      "Loss: 6.123021531336926\n",
      "T= 3000\n",
      "Loss: 6.123021522494552\n",
      "T= 3500\n",
      "Loss: 6.1230215221070345\n",
      "T= 4000\n",
      "Loss: 6.12302152208915\n",
      "T= 4500\n",
      "Loss: 6.123021522088305\n",
      "T= 5000\n",
      "Loss: 6.123021522088265\n",
      "T= 5500\n",
      "Loss: 6.123021522088262\n",
      "T= 6000\n",
      "Loss: 6.123021522088262\n",
      "T= 6500\n",
      "Loss: 6.123021522088262\n",
      "T= 7000\n",
      "Loss: 6.123021522088262\n",
      "T= 7500\n",
      "Loss: 6.123021522088262\n",
      "T= 8000\n",
      "Loss: 6.123021522088262\n",
      "T= 8500\n",
      "Loss: 6.123021522088262\n",
      "T= 9000\n",
      "Loss: 6.123021522088262\n",
      "T= 9500\n",
      "Loss: 6.123021522088262\n"
     ]
    }
   ],
   "source": [
    "dim = x.shape[1] + 1 \n",
    "w = np.zeros(shape = (dim, 1 ))\n",
    "x = np.concatenate((np.ones((x.shape[0], 1 )), x) , axis = 1).astype(float)\n",
    "learning_rate = np.array([[200]] * dim)\n",
    "adagrad_sum = np.zeros(shape = (dim, 1 ))\n",
    " \n",
    "for T in range(10000):\n",
    "    if(T % 500 == 0 ):\n",
    "        print(\"T=\",T)\n",
    "        print(\"Loss:\",np.power(np.sum(np.power(x.dot(w) - y, 2 ))/ x.shape[0],0.5))\n",
    "    gradient = (-2) * np.transpose(x).dot(y-x.dot(w))\n",
    "    adagrad_sum += gradient ** 2\n",
    "    w = w - learning_rate * gradient / (np.sqrt(adagrad_sum) + 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_data = np.genfromtxt('../input/ml2019spring-hw1/test.csv', encoding = 'unicode_escape', delimiter=',')   ## test.csv\n",
    "test_data = test_raw_data[:, 2: ]\n",
    "where_are_NaNs = np.isnan(test_data)\n",
    "test_data[where_are_NaNs] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.empty(shape = (240, 9),dtype = float)\n",
    "\n",
    "for i in range(240):\n",
    "    test_x[i,:] = test_data[i * 18 + 9,:].reshape(1,-1) \n",
    "\n",
    "for i in range(test_x.shape[0]):        ##Normalization\n",
    "    for j in range(test_x.shape[1]):\n",
    "        if not std[j] == 0 :\n",
    "            test_x[i][j] = (test_x[i][j]- mean[j]) / std[j]\n",
    "\n",
    "test_x = np.concatenate((np.ones(shape = (test_x.shape[0],1)),test_x),axis = 1).astype(float)\n",
    "answer = test_x.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('submission.csv',\"w\")\n",
    "w = csv.writer(f)\n",
    "title = ['id','value']\n",
    "w.writerow(title) \n",
    "for i in range(240):\n",
    "    content = ['id_'+str(i),answer[i][0]]\n",
    "    w.writerow(content) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> score:5.90263"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (2%)記錄誤差值 (RMSE)(根據kaggle public+private分數)，討論兩種feature的影響  \n",
    "  - 5.65650(使用全部) v.s. 5.90263(只使用PM2.5)\n",
    "  \n",
    "      只使用前PM2.5的在testing data表現比trainning好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (1%)將feature從抽前9小時改成抽前5小時，討論其變化\n",
    "\n",
    "#### (1) 抽全部5小時內的污染源feature當作一次項(加bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5700, 90), (5700, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = np.genfromtxt('../input/ml2019spring-hw1/train.csv', encoding = 'unicode_escape', delimiter=',') ## train.csv\n",
    "data = raw_data[1:,3:]\n",
    "where_are_NaNs = np.isnan(data)\n",
    "data[where_are_NaNs] = 0 \n",
    "\n",
    "month_to_data = {}  ## Dictionary (key:month , value:data)                                  \n",
    "\n",
    "for month in range(12):\n",
    "    sample = np.empty(shape = (18 , 480))\n",
    "    for day in range(20):\n",
    "        for hour in range(24): \n",
    "            sample[:,day * 24 + hour] = data[18 * (month * 20 + day): 18 * (month * 20 + day + 1),hour]\n",
    "    month_to_data[month] = sample  \n",
    "x = np.empty(shape = (12 * 475 , 18 * 5),dtype = float)\n",
    "y = np.empty(shape = (12 * 475 , 1),dtype = float)\n",
    "\n",
    "for month in range(12): \n",
    "    for day in range(20): \n",
    "        for hour in range(24):   \n",
    "            if day == 19 and hour > 18:\n",
    "                continue\n",
    "            x[month * 475 + day * 24 + hour,:] = month_to_data[month][:,day * 24 + hour : day * 24 + hour + 5].reshape(1,-1) \n",
    "            y[month * 475 + day * 24 + hour,0] = month_to_data[month][9 ,day * 24 + hour + 5]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(x, axis = 0) \n",
    "std = np.std(x, axis = 0)\n",
    "for i in range(x.shape[0]):\n",
    "    for j in range(x.shape[1]):\n",
    "        if not std[j] == 0 :\n",
    "            x[i][j] = (x[i][j]- mean[j]) / std[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T= 0\n",
      "Loss: 27.069300278061487\n",
      "T= 500\n",
      "Loss: 10.29073326042208\n",
      "T= 1000\n",
      "Loss: 7.046920069982195\n",
      "T= 1500\n",
      "Loss: 6.191543899341942\n",
      "T= 2000\n",
      "Loss: 5.937749267657019\n",
      "T= 2500\n",
      "Loss: 5.856301242070615\n",
      "T= 3000\n",
      "Loss: 5.827447154745556\n",
      "T= 3500\n",
      "Loss: 5.816217904712471\n",
      "T= 4000\n",
      "Loss: 5.811475826487308\n",
      "T= 4500\n",
      "Loss: 5.809318880574833\n",
      "T= 5000\n",
      "Loss: 5.808259666745374\n",
      "T= 5500\n",
      "Loss: 5.807691663026502\n",
      "T= 6000\n",
      "Loss: 5.8073542116274\n",
      "T= 6500\n",
      "Loss: 5.80713040739789\n",
      "T= 7000\n",
      "Loss: 5.806965800939391\n",
      "T= 7500\n",
      "Loss: 5.806834094386907\n",
      "T= 8000\n",
      "Loss: 5.806722107974225\n",
      "T= 8500\n",
      "Loss: 5.806622974002278\n",
      "T= 9000\n",
      "Loss: 5.8065329602006885\n",
      "T= 9500\n",
      "Loss: 5.8064499417239475\n"
     ]
    }
   ],
   "source": [
    "dim = x.shape[1] + 1 \n",
    "w = np.zeros(shape = (dim, 1 ))\n",
    "x = np.concatenate((np.ones((x.shape[0], 1 )), x) , axis = 1).astype(float)\n",
    "learning_rate = np.array([[200]] * dim)\n",
    "adagrad_sum = np.zeros(shape = (dim, 1 ))\n",
    " \n",
    "for T in range(10000):\n",
    "    if(T % 500 == 0 ):\n",
    "        print(\"T=\",T)\n",
    "        print(\"Loss:\",np.power(np.sum(np.power(x.dot(w) - y, 2 ))/ x.shape[0],0.5))\n",
    "    gradient = (-2) * np.transpose(x).dot(y-x.dot(w))\n",
    "    adagrad_sum += gradient ** 2\n",
    "    w = w - learning_rate * gradient / (np.sqrt(adagrad_sum) + 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_data = np.genfromtxt('../input/ml2019spring-hw1/test.csv', encoding = 'unicode_escape', delimiter=',')   ## test.csv\n",
    "test_data = test_raw_data[:, 2: ]\n",
    "where_are_NaNs = np.isnan(test_data)\n",
    "test_data[where_are_NaNs] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.empty(shape = (240, 18 * 5),dtype = float)\n",
    "\n",
    "for i in range(240):\n",
    "    test_x[i,:] = test_data[18 * i : 18 * (i+1),-5:].reshape(1,-1) \n",
    "\n",
    "for i in range(test_x.shape[0]):        ##Normalization\n",
    "    for j in range(test_x.shape[1]):\n",
    "        if not std[j] == 0 :\n",
    "            test_x[i][j] = (test_x[i][j]- mean[j]) / std[j]\n",
    "\n",
    "test_x = np.concatenate((np.ones(shape = (test_x.shape[0],1)),test_x),axis = 1).astype(float)\n",
    "answer = test_x.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('submission.csv',\"w\")\n",
    "w = csv.writer(f)\n",
    "title = ['id','value']\n",
    "w.writerow(title) \n",
    "for i in range(240):\n",
    "    content = ['id_'+str(i),answer[i][0]]\n",
    "    w.writerow(content) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> score:5.96405，train score與test score微幅上升\n",
    "\n",
    "#### (2) 抽全部5小時內pm2.5的一次項當作feature(加bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5700, 5), (5700, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = np.genfromtxt('../input/ml2019spring-hw1/train.csv', encoding = 'unicode_escape', delimiter=',') ## train.csv\n",
    "data = raw_data[1:,3:]\n",
    "where_are_NaNs = np.isnan(data)\n",
    "data[where_are_NaNs] = 0 \n",
    "\n",
    "month_to_data = {}  ## Dictionary (key:month , value:data)                                  \n",
    "\n",
    "for month in range(12):\n",
    "    sample = np.empty(shape = (18 , 480))\n",
    "    for day in range(20):\n",
    "        for hour in range(24): \n",
    "            sample[:,day * 24 + hour] = data[18 * (month * 20 + day): 18 * (month * 20 + day + 1),hour]\n",
    "    month_to_data[month] = sample  \n",
    "x = np.empty(shape = (12 * 475 , 5),dtype = float)\n",
    "y = np.empty(shape = (12 * 475 , 1),dtype = float)\n",
    "\n",
    "for month in range(12): \n",
    "    for day in range(20): \n",
    "        for hour in range(24):   \n",
    "            if day == 19 and hour > 14:\n",
    "                continue\n",
    "            x[month * 475 + day * 24 + hour,:] = month_to_data[month][9, day * 24 + hour : day * 24 + hour + 5].reshape(1,-1) \n",
    "            y[month * 475 + day * 24 + hour,0] = month_to_data[month][9, day * 24 + hour + 5]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(x, axis = 0) \n",
    "std = np.std(x, axis = 0)\n",
    "for i in range(x.shape[0]):\n",
    "    for j in range(x.shape[1]):\n",
    "        if not std[j] == 0 :\n",
    "            x[i][j] = (x[i][j]- mean[j]) / std[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T= 0\n",
      "Loss: 26.964730049949818\n",
      "T= 500\n",
      "Loss: 6.186472324045475\n",
      "T= 1000\n",
      "Loss: 6.1864213418364145\n",
      "T= 1500\n",
      "Loss: 6.186421337055632\n",
      "T= 2000\n",
      "Loss: 6.186421337055159\n",
      "T= 2500\n",
      "Loss: 6.186421337055159\n",
      "T= 3000\n",
      "Loss: 6.186421337055159\n",
      "T= 3500\n",
      "Loss: 6.18642133705516\n",
      "T= 4000\n",
      "Loss: 6.186421337055159\n",
      "T= 4500\n",
      "Loss: 6.186421337055159\n",
      "T= 5000\n",
      "Loss: 6.186421337055159\n",
      "T= 5500\n",
      "Loss: 6.186421337055159\n",
      "T= 6000\n",
      "Loss: 6.186421337055159\n",
      "T= 6500\n",
      "Loss: 6.186421337055159\n",
      "T= 7000\n",
      "Loss: 6.186421337055159\n",
      "T= 7500\n",
      "Loss: 6.186421337055159\n",
      "T= 8000\n",
      "Loss: 6.186421337055159\n",
      "T= 8500\n",
      "Loss: 6.186421337055159\n",
      "T= 9000\n",
      "Loss: 6.186421337055159\n",
      "T= 9500\n",
      "Loss: 6.186421337055159\n"
     ]
    }
   ],
   "source": [
    "dim = x.shape[1] + 1 \n",
    "w = np.zeros(shape = (dim, 1 ))\n",
    "x = np.concatenate((np.ones((x.shape[0], 1 )), x) , axis = 1).astype(float)\n",
    "learning_rate = np.array([[200]] * dim)\n",
    "adagrad_sum = np.zeros(shape = (dim, 1 ))\n",
    "for T in range(10000):\n",
    "    if(T % 500 == 0 ):\n",
    "        print(\"T=\",T)\n",
    "        print(\"Loss:\",np.power(np.sum(np.power(x.dot(w) - y, 2 ))/ x.shape[0],0.5))\n",
    "    gradient = (-2) * np.transpose(x).dot(y-x.dot(w))\n",
    "    adagrad_sum += gradient ** 2\n",
    "    w = w - learning_rate * gradient / (np.sqrt(adagrad_sum) + 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_data = np.genfromtxt('../input/ml2019spring-hw1/test.csv', encoding = 'unicode_escape', delimiter=',')   ## test.csv\n",
    "test_data = test_raw_data[:, 2: ]\n",
    "where_are_NaNs = np.isnan(test_data)\n",
    "test_data[where_are_NaNs] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.empty(shape = (240, 5),dtype = float)\n",
    "\n",
    "for i in range(240):\n",
    "    test_x[i,:] = test_data[i * 18 + 9,-5:].reshape(1,-1) \n",
    "\n",
    "for i in range(test_x.shape[0]):        ##Normalization\n",
    "    for j in range(test_x.shape[1]):\n",
    "        if not std[j] == 0 :\n",
    "            test_x[i][j] = (test_x[i][j]- mean[j]) / std[j]\n",
    "\n",
    "test_x = np.concatenate((np.ones(shape = (test_x.shape[0],1)),test_x),axis = 1).astype(float)\n",
    "answer = test_x.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('submission.csv',\"w\")\n",
    "w = csv.writer(f)\n",
    "title = ['id','value']\n",
    "w.writerow(title) \n",
    "for i in range(240):\n",
    "    content = ['id_'+str(i),answer[i][0]]\n",
    "    w.writerow(content) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> socre:6.22573，train score微幅上升，test score上升0.3，testing表現的較trainning差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (1%)Regularization on all the weight with λ=0.1、0.01、0.001、0.0001，並作圖\n",
    "\n",
    "=> 使用全部5小時內的污染源feature當作一次項"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T= 0\n",
      "Loss: 27.069300278061487\n",
      "T= 500\n",
      "Loss: 10.299676425778006\n",
      "T= 1000\n",
      "Loss: 7.051016523076887\n",
      "T= 1500\n",
      "Loss: 6.193819498905969\n",
      "T= 2000\n",
      "Loss: 5.939396562492837\n",
      "T= 2500\n",
      "Loss: 5.857721039466232\n",
      "T= 3000\n",
      "Loss: 5.828774533757347\n",
      "T= 3500\n",
      "Loss: 5.817502680502469\n",
      "T= 4000\n",
      "Loss: 5.812738638120796\n",
      "T= 4500\n",
      "Loss: 5.8105693106803065\n",
      "T= 5000\n",
      "Loss: 5.809502635782152\n",
      "T= 5500\n",
      "Loss: 5.808929921726902\n",
      "T= 6000\n",
      "Loss: 5.808589409424636\n",
      "T= 6500\n",
      "Loss: 5.808363591643837\n",
      "T= 7000\n",
      "Loss: 5.808197668805315\n",
      "T= 7500\n",
      "Loss: 5.8080651288321645\n",
      "T= 8000\n",
      "Loss: 5.807952655455644\n",
      "T= 8500\n",
      "Loss: 5.807853291005967\n",
      "T= 9000\n",
      "Loss: 5.807763242151428\n",
      "T= 9500\n",
      "Loss: 5.807680341473414\n",
      "T= 0\n",
      "Loss: 27.069300278061487\n",
      "T= 500\n",
      "Loss: 10.291627869229554\n",
      "T= 1000\n",
      "Loss: 7.047329752111883\n",
      "T= 1500\n",
      "Loss: 6.191771420131311\n",
      "T= 2000\n",
      "Loss: 5.937913937998011\n",
      "T= 2500\n",
      "Loss: 5.856443155806985\n",
      "T= 3000\n",
      "Loss: 5.827579823183915\n",
      "T= 3500\n",
      "Loss: 5.816346310369325\n",
      "T= 4000\n",
      "Loss: 5.8116020335700505\n",
      "T= 4500\n",
      "Loss: 5.809443847412385\n",
      "T= 5000\n",
      "Loss: 5.808383885362669\n",
      "T= 5500\n",
      "Loss: 5.807815408442561\n",
      "T= 6000\n",
      "Loss: 5.807477648720218\n",
      "T= 6500\n",
      "Loss: 5.807253640836131\n",
      "T= 7000\n",
      "Loss: 5.807088900377154\n",
      "T= 7500\n",
      "Loss: 5.806957108062855\n",
      "T= 8000\n",
      "Loss: 5.806845070483523\n",
      "T= 8500\n",
      "Loss: 5.806745910950124\n",
      "T= 9000\n",
      "Loss: 5.806655891093233\n",
      "T= 9500\n",
      "Loss: 5.806572881816891\n",
      "T= 0\n",
      "Loss: 27.069300278061487\n",
      "T= 500\n",
      "Loss: 10.290822724228137\n",
      "T= 1000\n",
      "Loss: 7.046961038563256\n",
      "T= 1500\n",
      "Loss: 6.191566651028545\n",
      "T= 2000\n",
      "Loss: 5.937765734098947\n",
      "T= 2500\n",
      "Loss: 5.856315432783482\n",
      "T= 3000\n",
      "Loss: 5.827460420894061\n",
      "T= 3500\n",
      "Loss: 5.8162307445582835\n",
      "T= 4000\n",
      "Loss: 5.8114884464541845\n",
      "T= 4500\n",
      "Loss: 5.809331376496334\n",
      "T= 5000\n",
      "Loss: 5.808272087823791\n",
      "T= 5500\n",
      "Loss: 5.807704036763199\n",
      "T= 6000\n",
      "Loss: 5.807366554509529\n",
      "T= 6500\n",
      "Loss: 5.807142729891656\n",
      "T= 7000\n",
      "Loss: 5.806978110009577\n",
      "T= 7500\n",
      "Loss: 5.80684639485681\n",
      "T= 8000\n",
      "Loss: 5.806734403302859\n",
      "T= 8500\n",
      "Loss: 5.806635266749731\n",
      "T= 9000\n",
      "Loss: 5.806545252317215\n",
      "T= 9500\n",
      "Loss: 5.8064622347348225\n",
      "T= 0\n",
      "Loss: 27.069300278061487\n",
      "T= 500\n",
      "Loss: 10.290742206831935\n",
      "T= 1000\n",
      "Loss: 7.046924166843981\n",
      "T= 1500\n",
      "Loss: 6.191546174506679\n",
      "T= 2000\n",
      "Loss: 5.93775091429529\n",
      "T= 2500\n",
      "Loss: 5.8563026611352935\n",
      "T= 3000\n",
      "Loss: 5.8274484813534535\n",
      "T= 3500\n",
      "Loss: 5.816219188689853\n",
      "T= 4000\n",
      "Loss: 5.811477088476581\n",
      "T= 4500\n",
      "Loss: 5.809320130159359\n",
      "T= 5000\n",
      "Loss: 5.808260908845383\n",
      "T= 5500\n",
      "Loss: 5.807692900392122\n",
      "T= 6000\n",
      "Loss: 5.807355445907341\n",
      "T= 6500\n",
      "Loss: 5.807131639638767\n",
      "T= 7000\n",
      "Loss: 5.806967031837674\n",
      "T= 7500\n",
      "Loss: 5.80683532442492\n",
      "T= 8000\n",
      "Loss: 5.806723337497865\n",
      "T= 8500\n",
      "Loss: 5.806624203267549\n",
      "T= 9000\n",
      "Loss: 5.806534189402615\n",
      "T= 9500\n",
      "Loss: 5.806451171015052\n",
      "T= 0\n",
      "Loss: 27.069300278061487\n",
      "T= 500\n",
      "Loss: 10.29073326042208\n",
      "T= 1000\n",
      "Loss: 7.046920069982195\n",
      "T= 1500\n",
      "Loss: 6.191543899341942\n",
      "T= 2000\n",
      "Loss: 5.937749267657019\n",
      "T= 2500\n",
      "Loss: 5.856301242070615\n",
      "T= 3000\n",
      "Loss: 5.827447154745556\n",
      "T= 3500\n",
      "Loss: 5.816217904712471\n",
      "T= 4000\n",
      "Loss: 5.811475826487308\n",
      "T= 4500\n",
      "Loss: 5.809318880574833\n",
      "T= 5000\n",
      "Loss: 5.808259666745374\n",
      "T= 5500\n",
      "Loss: 5.807691663026502\n",
      "T= 6000\n",
      "Loss: 5.8073542116274\n",
      "T= 6500\n",
      "Loss: 5.80713040739789\n",
      "T= 7000\n",
      "Loss: 5.806965800939391\n",
      "T= 7500\n",
      "Loss: 5.806834094386907\n",
      "T= 8000\n",
      "Loss: 5.806722107974225\n",
      "T= 8500\n",
      "Loss: 5.806622974002278\n",
      "T= 9000\n",
      "Loss: 5.8065329602006885\n",
      "T= 9500\n",
      "Loss: 5.8064499417239475\n"
     ]
    }
   ],
   "source": [
    "raw_data = np.genfromtxt('../input/ml2019spring-hw1/train.csv', encoding = 'unicode_escape', delimiter=',') ## train.csv\n",
    "data = raw_data[1:,3:]\n",
    "where_are_NaNs = np.isnan(data)\n",
    "data[where_are_NaNs] = 0 \n",
    "\n",
    "month_to_data = {}  ## Dictionary (key:month , value:data)                                  \n",
    "\n",
    "for month in range(12):\n",
    "    sample = np.empty(shape = (18 , 480))\n",
    "    for day in range(20):\n",
    "        for hour in range(24): \n",
    "            sample[:,day * 24 + hour] = data[18 * (month * 20 + day): 18 * (month * 20 + day + 1),hour]\n",
    "    month_to_data[month] = sample  \n",
    "lamdas = [0.1, 0.01, 0.001, 0.0001, 0]\n",
    "loss_by_lamda = []\n",
    "w_by_lamda = []\n",
    " \n",
    "for index, lamda in enumerate(lamdas):\n",
    "    x = np.empty(shape = (12 * 475 , 18 * 5),dtype = float)\n",
    "    y = np.empty(shape = (12 * 475 , 1),dtype = float)\n",
    "    for month in range(12): \n",
    "        for day in range(20): \n",
    "            for hour in range(24):   \n",
    "                if day == 19 and hour > 18:\n",
    "                    continue\n",
    "                x[month * 475 + day * 24 + hour,:] = month_to_data[month][:,day * 24 + hour : day * 24 + hour + 5].reshape(1,-1) \n",
    "                y[month * 475 + day * 24 + hour,0] = month_to_data[month][9 ,day * 24 + hour + 5]\n",
    "    mean = np.mean(x, axis = 0) \n",
    "    std = np.std(x, axis = 0)\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if not std[j] == 0 :\n",
    "                x[i][j] = (x[i][j]- mean[j]) / std[j]\n",
    "    dim = x.shape[1] + 1 \n",
    "    w = np.zeros(shape = (dim, 1 ))\n",
    "    x = np.concatenate((np.ones((x.shape[0], 1 )), x) , axis = 1).astype(float)\n",
    "    learning_rate = np.array([[200]] * dim)\n",
    "    adagrad_sum = np.zeros(shape = (dim, 1 ))\n",
    "    loss = []\n",
    "    for T in range(10000):\n",
    "        if(T % 500 == 0 ):\n",
    "            print(\"T=\",T)\n",
    "            print(\"Loss:\", np.power((np.sum(np.power(x.dot(w) - y, 2 )) + lamda * np.sum(np.power(w, 2)))/ x.shape[0], 0.5))\n",
    "        loss.append(np.power((np.sum(np.power(x.dot(w) - y, 2 )) + lamda * np.sum(np.power(w, 2)))/ x.shape[0], 0.5))\n",
    "        gradient = (-2) * np.transpose(x).dot(y-x.dot(w)) + 2 * lamda * np.sum(w)\n",
    "        adagrad_sum += gradient ** 2\n",
    "        w = w - learning_rate * gradient / (np.sqrt(adagrad_sum) + 0.0005)\n",
    "    loss_by_lamda.append(loss[-1])\n",
    "    w_by_lamda.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_data = np.genfromtxt('../input/ml2019spring-hw1/test.csv', encoding = 'unicode_escape', delimiter=',')   ## test.csv\n",
    "test_data = test_raw_data[:, 2: ]\n",
    "where_are_NaNs = np.isnan(test_data)\n",
    "test_data[where_are_NaNs] = 0 \n",
    "test_x = np.empty(shape = (240, 18 * 5),dtype = float)\n",
    "\n",
    "for i in range(240):\n",
    "    test_x[i,:] = test_data[18 * i : 18 * (i+1),-5:].reshape(1,-1) \n",
    "\n",
    "for i in range(test_x.shape[0]):        ##Normalization\n",
    "    for j in range(test_x.shape[1]):\n",
    "        if not std[j] == 0 :\n",
    "            test_x[i][j] = (test_x[i][j]- mean[j]) / std[j]\n",
    "\n",
    "test_x = np.concatenate((np.ones(shape = (test_x.shape[0],1)),test_x),axis = 1).astype(float)\n",
    "\n",
    "for i, w in enumerate(w_by_lamda):\n",
    "    answer = test_x.dot(w)\n",
    "    f = open('submission{}.csv'.format(i),\"w\")\n",
    "    wr = csv.writer(f)\n",
    "    title = ['id','value']\n",
    "    wr.writerow(title) \n",
    "    for i in range(240):\n",
    "        content = ['id_'+str(i),answer[i][0]]\n",
    "        wr.writerow(content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(list(range(5)), loss_by_lamda)\n",
    "ax.plot(list(range(5)), [5.96438, 5.96408, 5.96405, 5.96405, 5.96405])\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "plt.ylim((5.6, 6)) \n",
    "plt.xticks(list(range(5)), [str(i) for i in lamdas])\n",
    "plt.xlabel('λ')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果並不明顯  \n",
    "=> 使用λ=1000、100、10、1、0重做一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T= 0\n",
      "Loss: 27.069300278061487\n",
      "T= 500\n",
      "Loss: 104.11683222948739\n",
      "T= 1000\n",
      "Loss: 78.63308195000154\n",
      "T= 1500\n",
      "Loss: 63.007471483476046\n",
      "T= 2000\n",
      "Loss: 51.99343124814391\n",
      "T= 2500\n",
      "Loss: 43.71881459839402\n",
      "T= 3000\n",
      "Loss: 37.38902392202585\n",
      "T= 3500\n",
      "Loss: 32.56819858087462\n",
      "T= 4000\n",
      "Loss: 28.960343398000475\n",
      "T= 4500\n",
      "Loss: 26.33613852496346\n",
      "T= 5000\n",
      "Loss: 24.50431960009465\n",
      "T= 5500\n",
      "Loss: 23.3000418069747\n",
      "T= 6000\n",
      "Loss: 22.581366740707395\n",
      "T= 6500\n",
      "Loss: 22.229065496852687\n",
      "T= 7000\n",
      "Loss: 22.146500267470994\n",
      "T= 7500\n",
      "Loss: 22.258127972933995\n",
      "T= 8000\n",
      "Loss: 22.50667983787789\n",
      "T= 8500\n",
      "Loss: 22.849732828474032\n",
      "T= 9000\n",
      "Loss: 23.256355946736154\n",
      "T= 9500\n",
      "Loss: 23.704221384312326\n",
      "T= 0\n",
      "Loss: 27.069300278061487\n",
      "T= 500\n",
      "Loss: 17.65201815818875\n",
      "T= 1000\n",
      "Loss: 10.879797750068596\n",
      "T= 1500\n",
      "Loss: 8.602453462172985\n",
      "T= 2000\n",
      "Loss: 7.825629665174065\n",
      "T= 2500\n",
      "Loss: 7.562829681667214\n",
      "T= 3000\n",
      "Loss: 7.478608475075066\n",
      "T= 3500\n",
      "Loss: 7.462272610430795\n",
      "T= 4000\n",
      "Loss: 7.475243454608394\n",
      "T= 4500\n",
      "Loss: 7.502705580493934\n",
      "T= 5000\n",
      "Loss: 7.538295451679559\n",
      "T= 5500\n",
      "Loss: 7.578938996179787\n",
      "T= 6000\n",
      "Loss: 7.622963268209715\n",
      "T= 6500\n",
      "Loss: 7.669347834788646\n",
      "T= 7000\n",
      "Loss: 7.7174051414166325\n",
      "T= 7500\n",
      "Loss: 7.766634137660568\n",
      "T= 8000\n",
      "Loss: 7.8166483926366235\n",
      "T= 8500\n",
      "Loss: 7.867138151630023\n",
      "T= 9000\n",
      "Loss: 7.91784877260913\n",
      "T= 9500\n",
      "Loss: 7.968567545438548\n",
      "T= 0\n",
      "Loss: 27.069300278061487\n",
      "T= 500\n",
      "Loss: 11.155534012716128\n",
      "T= 1000\n",
      "Loss: 7.452444734795216\n",
      "T= 1500\n",
      "Loss: 6.422745727445219\n",
      "T= 2000\n",
      "Loss: 6.108220093141875\n",
      "T= 2500\n",
      "Loss: 6.004778694685317\n",
      "T= 3000\n",
      "Loss: 5.967096206279354\n",
      "T= 3500\n",
      "Loss: 5.951925130672633\n",
      "T= 4000\n",
      "Loss: 5.945281840508972\n",
      "T= 4500\n",
      "Loss: 5.9421821653770355\n",
      "T= 5000\n",
      "Loss: 5.94068074556344\n",
      "T= 5500\n",
      "Loss: 5.939956811116678\n",
      "T= 6000\n",
      "Loss: 5.939640052974154\n",
      "T= 6500\n",
      "Loss: 5.9395528506308635\n",
      "T= 7000\n",
      "Loss: 5.9396047838385755\n",
      "T= 7500\n",
      "Loss: 5.939746988938319\n",
      "T= 8000\n",
      "Loss: 5.939951427142459\n",
      "T= 8500\n",
      "Loss: 5.940201027958304\n",
      "T= 9000\n",
      "Loss: 5.940484786420098\n",
      "T= 9500\n",
      "Loss: 5.9407952121408565\n",
      "T= 0\n",
      "Loss: 27.069300278061487\n",
      "T= 500\n",
      "Loss: 10.379875200953386\n",
      "T= 1000\n",
      "Loss: 7.087847676662629\n",
      "T= 1500\n",
      "Loss: 6.214338403683794\n",
      "T= 2000\n",
      "Loss: 5.954280617918369\n",
      "T= 2500\n",
      "Loss: 5.8705644859675505\n",
      "T= 3000\n",
      "Loss: 5.840789709988659\n",
      "T= 3500\n",
      "Loss: 5.82913693680603\n",
      "T= 4000\n",
      "Loss: 5.824177434850325\n",
      "T= 4500\n",
      "Loss: 5.8218988330204\n",
      "T= 5000\n",
      "Loss: 5.820767195260779\n",
      "T= 5500\n",
      "Loss: 5.820154334664779\n",
      "T= 6000\n",
      "Loss: 5.8197885908485985\n",
      "T= 6500\n",
      "Loss: 5.819547038420028\n",
      "T= 7000\n",
      "Loss: 5.819371722108758\n",
      "T= 7500\n",
      "Loss: 5.8192341956677565\n",
      "T= 8000\n",
      "Loss: 5.819119907078331\n",
      "T= 8500\n",
      "Loss: 5.81902108139988\n",
      "T= 9000\n",
      "Loss: 5.818933368268332\n",
      "T= 9500\n",
      "Loss: 5.818854209881239\n",
      "T= 0\n",
      "Loss: 27.069300278061487\n",
      "T= 500\n",
      "Loss: 10.29073326042208\n",
      "T= 1000\n",
      "Loss: 7.046920069982195\n",
      "T= 1500\n",
      "Loss: 6.191543899341942\n",
      "T= 2000\n",
      "Loss: 5.937749267657019\n",
      "T= 2500\n",
      "Loss: 5.856301242070615\n",
      "T= 3000\n",
      "Loss: 5.827447154745556\n",
      "T= 3500\n",
      "Loss: 5.816217904712471\n",
      "T= 4000\n",
      "Loss: 5.811475826487308\n",
      "T= 4500\n",
      "Loss: 5.809318880574833\n",
      "T= 5000\n",
      "Loss: 5.808259666745374\n",
      "T= 5500\n",
      "Loss: 5.807691663026502\n",
      "T= 6000\n",
      "Loss: 5.8073542116274\n",
      "T= 6500\n",
      "Loss: 5.80713040739789\n",
      "T= 7000\n",
      "Loss: 5.806965800939391\n",
      "T= 7500\n",
      "Loss: 5.806834094386907\n",
      "T= 8000\n",
      "Loss: 5.806722107974225\n",
      "T= 8500\n",
      "Loss: 5.806622974002278\n",
      "T= 9000\n",
      "Loss: 5.8065329602006885\n",
      "T= 9500\n",
      "Loss: 5.8064499417239475\n"
     ]
    }
   ],
   "source": [
    "raw_data = np.genfromtxt('../input/ml2019spring-hw1/train.csv', encoding = 'unicode_escape', delimiter=',') ## train.csv\n",
    "data = raw_data[1:,3:]\n",
    "where_are_NaNs = np.isnan(data)\n",
    "data[where_are_NaNs] = 0 \n",
    "\n",
    "month_to_data = {}  ## Dictionary (key:month , value:data)                                  \n",
    "\n",
    "for month in range(12):\n",
    "    sample = np.empty(shape = (18 , 480))\n",
    "    for day in range(20):\n",
    "        for hour in range(24): \n",
    "            sample[:,day * 24 + hour] = data[18 * (month * 20 + day): 18 * (month * 20 + day + 1),hour]\n",
    "    month_to_data[month] = sample  \n",
    "lamdas = [1000, 100, 10, 1, 0]\n",
    "loss_by_lamda = []\n",
    "w_by_lamda = []\n",
    " \n",
    "for index, lamda in enumerate(lamdas):\n",
    "    x = np.empty(shape = (12 * 475 , 18 * 5),dtype = float)\n",
    "    y = np.empty(shape = (12 * 475 , 1),dtype = float)\n",
    "    for month in range(12): \n",
    "        for day in range(20): \n",
    "            for hour in range(24):   \n",
    "                if day == 19 and hour > 18:\n",
    "                    continue\n",
    "                x[month * 475 + day * 24 + hour,:] = month_to_data[month][:,day * 24 + hour : day * 24 + hour + 5].reshape(1,-1) \n",
    "                y[month * 475 + day * 24 + hour,0] = month_to_data[month][9 ,day * 24 + hour + 5]\n",
    "    mean = np.mean(x, axis = 0) \n",
    "    std = np.std(x, axis = 0)\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if not std[j] == 0 :\n",
    "                x[i][j] = (x[i][j]- mean[j]) / std[j]\n",
    "    dim = x.shape[1] + 1 \n",
    "    w = np.zeros(shape = (dim, 1 ))\n",
    "    x = np.concatenate((np.ones((x.shape[0], 1 )), x) , axis = 1).astype(float)\n",
    "    learning_rate = np.array([[200]] * dim)\n",
    "    adagrad_sum = np.zeros(shape = (dim, 1 ))\n",
    "    loss = []\n",
    "    for T in range(10000):\n",
    "        if(T % 500 == 0 ):\n",
    "            print(\"T=\",T)\n",
    "            print(\"Loss:\", np.power((np.sum(np.power(x.dot(w) - y, 2 )) + lamda * np.sum(np.power(w, 2)))/ x.shape[0], 0.5))\n",
    "        loss.append(np.power((np.sum(np.power(x.dot(w) - y, 2 )) + lamda * np.sum(np.power(w, 2)))/ x.shape[0], 0.5))\n",
    "        gradient = (-2) * np.transpose(x).dot(y-x.dot(w)) + 2 * lamda * np.sum(w)\n",
    "        adagrad_sum += gradient ** 2\n",
    "        w = w - learning_rate * gradient / (np.sqrt(adagrad_sum) + 0.0005)\n",
    "    loss_by_lamda.append(loss[-1])\n",
    "    w_by_lamda.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_data = np.genfromtxt('../input/ml2019spring-hw1/test.csv', encoding = 'unicode_escape', delimiter=',')   ## test.csv\n",
    "test_data = test_raw_data[:, 2: ]\n",
    "where_are_NaNs = np.isnan(test_data)\n",
    "test_data[where_are_NaNs] = 0 \n",
    "test_x = np.empty(shape = (240, 18 * 5),dtype = float)\n",
    "\n",
    "for i in range(240):\n",
    "    test_x[i,:] = test_data[18 * i : 18 * (i+1),-5:].reshape(1,-1) \n",
    "\n",
    "for i in range(test_x.shape[0]):        ##Normalization\n",
    "    for j in range(test_x.shape[1]):\n",
    "        if not std[j] == 0 :\n",
    "            test_x[i][j] = (test_x[i][j]- mean[j]) / std[j]\n",
    "\n",
    "test_x = np.concatenate((np.ones(shape = (test_x.shape[0],1)),test_x),axis = 1).astype(float)\n",
    "\n",
    "for i, w in enumerate(w_by_lamda):\n",
    "    answer = test_x.dot(w)\n",
    "    f = open('submission{}.csv'.format(i),\"w\")\n",
    "    wr = csv.writer(f)\n",
    "    title = ['id','value']\n",
    "    wr.writerow(title) \n",
    "    for i in range(240):\n",
    "        content = ['id_'+str(i),answer[i][0]]\n",
    "        wr.writerow(content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(list(range(5)), loss_by_lamda[::-1])\n",
    "ax.plot(list(range(5)), [9.11374, 6.91628, 6.01404, 5.96748, 5.96405][::-1])\n",
    "\n",
    "#### (2) 抽全部5小時內pm2.5的一次項當作feature(加bias)])\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "plt.xticks(list(range(5)), [str(i) for i in lamdas][::-1])\n",
    "plt.xlabel('λ')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用9小時內PM2.5一次項作為feature重做一次(λ=0.1、0.01、0.001、0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5652, 9) (5652, 1)\n",
      "T= 0\n",
      "Loss: 27.071214829194115\n",
      "T= 500\n",
      "Loss: 6.134600371278934\n",
      "T= 1000\n",
      "Loss: 6.12468877736046\n",
      "T= 1500\n",
      "Loss: 6.1244535590750795\n",
      "T= 2000\n",
      "Loss: 6.124446999386097\n",
      "T= 2500\n",
      "Loss: 6.124446778654558\n",
      "T= 3000\n",
      "Loss: 6.124446769934171\n",
      "T= 3500\n",
      "Loss: 6.124446769552998\n",
      "T= 4000\n",
      "Loss: 6.12444676953545\n",
      "T= 4500\n",
      "Loss: 6.124446769534624\n",
      "T= 5000\n",
      "Loss: 6.124446769534584\n",
      "T= 5500\n",
      "Loss: 6.124446769534583\n",
      "T= 6000\n",
      "Loss: 6.124446769534583\n",
      "T= 6500\n",
      "Loss: 6.124446769534583\n",
      "T= 7000\n",
      "Loss: 6.124446769534583\n",
      "T= 7500\n",
      "Loss: 6.124446769534583\n",
      "T= 8000\n",
      "Loss: 6.1244467695345834\n",
      "T= 8500\n",
      "Loss: 6.124446769534582\n",
      "T= 9000\n",
      "Loss: 6.124446769534583\n",
      "T= 9500\n",
      "Loss: 6.124446769534583\n",
      "(5652, 9) (5652, 1)\n",
      "T= 0\n",
      "Loss: 27.071214829194115\n",
      "T= 500\n",
      "Loss: 6.133349397239059\n",
      "T= 1000\n",
      "Loss: 6.123407420283097\n",
      "T= 1500\n",
      "Loss: 6.123170930018943\n",
      "T= 2000\n",
      "Loss: 6.1231643192666985\n",
      "T= 2500\n",
      "Loss: 6.1231640962872\n",
      "T= 3000\n",
      "Loss: 6.123164087457101\n",
      "T= 3500\n",
      "Loss: 6.123164087070223\n",
      "T= 4000\n",
      "Loss: 6.1231640870523725\n",
      "T= 4500\n",
      "Loss: 6.123164087051529\n",
      "T= 5000\n",
      "Loss: 6.123164087051489\n",
      "T= 5500\n",
      "Loss: 6.123164087051486\n",
      "T= 6000\n",
      "Loss: 6.123164087051486\n",
      "T= 6500\n",
      "Loss: 6.123164087051486\n",
      "T= 7000\n",
      "Loss: 6.123164087051486\n",
      "T= 7500\n",
      "Loss: 6.123164087051486\n",
      "T= 8000\n",
      "Loss: 6.123164087051486\n",
      "T= 8500\n",
      "Loss: 6.123164087051486\n",
      "T= 9000\n",
      "Loss: 6.123164087051486\n",
      "T= 9500\n",
      "Loss: 6.123164087051486\n",
      "(5652, 9) (5652, 1)\n",
      "T= 0\n",
      "Loss: 27.071214829194115\n",
      "T= 500\n",
      "Loss: 6.133224265765655\n",
      "T= 1000\n",
      "Loss: 6.123279245165973\n",
      "T= 1500\n",
      "Loss: 6.12304262732048\n",
      "T= 2000\n",
      "Loss: 6.123036011439925\n",
      "T= 2500\n",
      "Loss: 6.123035788234369\n",
      "T= 3000\n",
      "Loss: 6.123035779393223\n",
      "T= 3500\n",
      "Loss: 6.12303577900577\n",
      "T= 4000\n",
      "Loss: 6.123035778987888\n",
      "T= 4500\n",
      "Loss: 6.1230357789870435\n",
      "T= 5000\n",
      "Loss: 6.123035778987003\n",
      "T= 5500\n",
      "Loss: 6.123035778987001\n",
      "T= 6000\n",
      "Loss: 6.123035778987001\n",
      "T= 6500\n",
      "Loss: 6.123035778987001\n",
      "T= 7000\n",
      "Loss: 6.123035778987001\n",
      "T= 7500\n",
      "Loss: 6.123035778987\n",
      "T= 8000\n",
      "Loss: 6.123035778987001\n",
      "T= 8500\n",
      "Loss: 6.123035778987001\n",
      "T= 9000\n",
      "Loss: 6.123035778987001\n",
      "T= 9500\n",
      "Loss: 6.123035778987001\n",
      "(5652, 9) (5652, 1)\n",
      "T= 0\n",
      "Loss: 27.071214829194115\n",
      "T= 500\n",
      "Loss: 6.133211752277465\n",
      "T= 1000\n",
      "Loss: 6.123266427259945\n",
      "T= 1500\n",
      "Loss: 6.123029796652475\n",
      "T= 2000\n",
      "Loss: 6.12302318025887\n",
      "T= 2500\n",
      "Loss: 6.123022957030695\n",
      "T= 3000\n",
      "Loss: 6.123022948188444\n",
      "T= 3500\n",
      "Loss: 6.1230229478009335\n",
      "T= 4000\n",
      "Loss: 6.123022947783048\n",
      "T= 4500\n",
      "Loss: 6.123022947782203\n",
      "T= 5000\n",
      "Loss: 6.123022947782163\n",
      "T= 5500\n",
      "Loss: 6.123022947782161\n",
      "T= 6000\n",
      "Loss: 6.123022947782161\n",
      "T= 6500\n",
      "Loss: 6.123022947782161\n",
      "T= 7000\n",
      "Loss: 6.12302294778216\n",
      "T= 7500\n",
      "Loss: 6.12302294778216\n",
      "T= 8000\n",
      "Loss: 6.123022947782161\n",
      "T= 8500\n",
      "Loss: 6.12302294778216\n",
      "T= 9000\n",
      "Loss: 6.123022947782161\n",
      "T= 9500\n",
      "Loss: 6.123022947782161\n",
      "(5652, 9) (5652, 1)\n",
      "T= 0\n",
      "Loss: 27.071214829194115\n",
      "T= 500\n",
      "Loss: 6.1332103618860625\n",
      "T= 1000\n",
      "Loss: 6.123265003043738\n",
      "T= 1500\n",
      "Loss: 6.123028371018228\n",
      "T= 2000\n",
      "Loss: 6.123021754567614\n",
      "T= 2500\n",
      "Loss: 6.123021531336926\n",
      "T= 3000\n",
      "Loss: 6.123021522494552\n",
      "T= 3500\n",
      "Loss: 6.1230215221070345\n",
      "T= 4000\n",
      "Loss: 6.12302152208915\n",
      "T= 4500\n",
      "Loss: 6.123021522088305\n",
      "T= 5000\n",
      "Loss: 6.123021522088265\n",
      "T= 5500\n",
      "Loss: 6.123021522088262\n",
      "T= 6000\n",
      "Loss: 6.123021522088262\n",
      "T= 6500\n",
      "Loss: 6.123021522088262\n",
      "T= 7000\n",
      "Loss: 6.123021522088262\n",
      "T= 7500\n",
      "Loss: 6.123021522088262\n",
      "T= 8000\n",
      "Loss: 6.123021522088262\n",
      "T= 8500\n",
      "Loss: 6.123021522088262\n",
      "T= 9000\n",
      "Loss: 6.123021522088262\n",
      "T= 9500\n",
      "Loss: 6.123021522088262\n"
     ]
    }
   ],
   "source": [
    "raw_data = np.genfromtxt('../input/ml2019spring-hw1/train.csv', encoding = 'unicode_escape', delimiter=',') ## train.csv\n",
    "data = raw_data[1:,3:]\n",
    "where_are_NaNs = np.isnan(data)\n",
    "data[where_are_NaNs] = 0 \n",
    "\n",
    "month_to_data = {}  ## Dictionary (key:month , value:data)                                  \n",
    "\n",
    "for month in range(12):\n",
    "    sample = np.empty(shape = (18 , 480))\n",
    "    for day in range(20):\n",
    "        for hour in range(24): \n",
    "            sample[:,day * 24 + hour] = data[18 * (month * 20 + day): 18 * (month * 20 + day + 1),hour]\n",
    "    month_to_data[month] = sample  \n",
    "    \n",
    "lamdas = [0.1, 0.01, 0.001, 0.0001, 0]\n",
    "loss_by_lamda = []\n",
    "w_by_lamda = []\n",
    " \n",
    "for index, lamda in enumerate(lamdas):\n",
    "    x = np.empty(shape = (12 * 471 , 9),dtype = float)\n",
    "    y = np.empty(shape = (12 * 471 , 1),dtype = float)\n",
    "    for month in range(12): \n",
    "        for day in range(20): \n",
    "            for hour in range(24):   \n",
    "                if day == 19 and hour > 14:\n",
    "                    continue\n",
    "                x[month * 471 + day * 24 + hour,:] = month_to_data[month][9, day * 24 + hour : day * 24 + hour + 9].reshape(1,-1) \n",
    "                y[month * 471 + day * 24 + hour,0] = month_to_data[month][9, day * 24 + hour + 9]\n",
    "    print(x.shape, y.shape)\n",
    "    mean = np.mean(x, axis = 0) \n",
    "    std = np.std(x, axis = 0)\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if not std[j] == 0 :\n",
    "                x[i][j] = (x[i][j]- mean[j]) / std[j]\n",
    "    dim = x.shape[1] + 1 \n",
    "    w = np.zeros(shape = (dim, 1 ))\n",
    "    x = np.concatenate((np.ones((x.shape[0], 1 )), x) , axis = 1).astype(float)\n",
    "    learning_rate = np.array([[200]] * dim)\n",
    "    adagrad_sum = np.zeros(shape = (dim, 1 ))\n",
    "    loss = []\n",
    "    for T in range(10000):\n",
    "        if(T % 500 == 0 ):\n",
    "            print(\"T=\",T)\n",
    "            print(\"Loss:\", np.power((np.sum(np.power(x.dot(w) - y, 2 )) + lamda * np.sum(np.power(w, 2)))/ x.shape[0], 0.5))\n",
    "        loss.append(np.power((np.sum(np.power(x.dot(w) - y, 2 )) + lamda * np.sum(np.power(w, 2)))/ x.shape[0], 0.5))\n",
    "        gradient = (-2) * np.transpose(x).dot(y-x.dot(w)) + 2 * lamda * w #use np.sum(w) or w ? https://ntumlta2019.github.io/ml-web-hw2/LogisticRegression.html\n",
    "        adagrad_sum += gradient ** 2\n",
    "        w = w - learning_rate * gradient / (np.sqrt(adagrad_sum) + 0.0005)\n",
    "    loss_by_lamda.append(loss[-1])\n",
    "    w_by_lamda.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_data = np.genfromtxt('../input/ml2019spring-hw1/test.csv', encoding = 'unicode_escape', delimiter=',')   ## test.csv\n",
    "test_data = test_raw_data[:, 2: ]\n",
    "where_are_NaNs = np.isnan(test_data)\n",
    "test_data[where_are_NaNs] = 0 \n",
    "test_x = np.empty(shape = (240, 9),dtype = float)\n",
    "\n",
    "for i in range(240):\n",
    "    test_x[i,:] = test_data[i * 18 + 9,:].reshape(1,-1) \n",
    "\n",
    "for i in range(test_x.shape[0]):        ##Normalization\n",
    "    for j in range(test_x.shape[1]):\n",
    "        if not std[j] == 0 :\n",
    "            test_x[i][j] = (test_x[i][j]- mean[j]) / std[j]\n",
    "\n",
    "test_x = np.concatenate((np.ones(shape = (test_x.shape[0],1)),test_x),axis = 1).astype(float)\n",
    "\n",
    "for i, w in enumerate(w_by_lamda):\n",
    "    answer = test_x.dot(w)\n",
    "    f = open('submission{}.csv'.format(i),\"w\")\n",
    "    wr = csv.writer(f)\n",
    "    title = ['id','value']\n",
    "    wr.writerow(title) \n",
    "    for i in range(240):\n",
    "        content = ['id_'+str(i),answer[i][0]]\n",
    "        wr.writerow(content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(list(range(5)), loss_by_lamda)\n",
    "ax.plot(list(range(5)), [5.90266, 5.90263, 5.90263, 5.90263, 5.90263])\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "plt.xticks(list(range(5)), [str(i) for i in lamdas])\n",
    "plt.xlabel('λ')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
